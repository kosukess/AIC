{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import json\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg \n",
    "import trt_pose.coco\n",
    "import math\n",
    "import os\n",
    "import numpy as np\n",
    "import traitlets\n",
    "import pickle \n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "with open('preprocess/hand_pose.json', 'r') as f:\n",
    "    hand_pose = json.load(f)\n",
    "\n",
    "topology = trt_pose.coco.coco_category_to_topology(hand_pose)\n",
    "import trt_pose.models\n",
    "\n",
    "num_parts = len(hand_pose['keypoints'])\n",
    "num_links = len(hand_pose['skeleton'])\n",
    "\n",
    "model = trt_pose.models.resnet18_baseline_att(num_parts, 2 * num_links).cuda().eval()\n",
    "import torch\n",
    "\n",
    "\n",
    "WIDTH = 224\n",
    "HEIGHT = 224\n",
    "data = torch.zeros((1, 3, HEIGHT, WIDTH)).cuda()\n",
    "\n",
    "if not os.path.exists('model/hand_pose_resnet18_att_244_244_trt.pth'):\n",
    "    MODEL_WEIGHTS = 'model/hand_pose_resnet18_att_244_244.pth'\n",
    "    model.load_state_dict(torch.load(MODEL_WEIGHTS))\n",
    "    import torch2trt\n",
    "    model_trt = torch2trt.torch2trt(model, [data], fp16_mode=True, max_workspace_size=1<<25)\n",
    "    OPTIMIZED_MODEL = 'model/hand_pose_resnet18_att_244_244_trt.pth'\n",
    "    torch.save(model_trt.state_dict(), OPTIMIZED_MODEL)\n",
    "\n",
    "\n",
    "OPTIMIZED_MODEL = 'model/hand_pose_resnet18_att_244_244_trt.pth'\n",
    "from torch2trt import TRTModule\n",
    "\n",
    "model_trt = TRTModule()\n",
    "model_trt.load_state_dict(torch.load(OPTIMIZED_MODEL))"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "from trt_pose.draw_objects import DrawObjects\n",
    "from trt_pose.parse_objects import ParseObjects\n",
    "\n",
    "parse_objects = ParseObjects(topology,cmap_threshold=0.12, link_threshold=0.15)\n",
    "draw_objects = DrawObjects(topology)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "\n",
    "import torchvision.transforms as transforms\n",
    "import PIL.Image\n",
    "\n",
    "mean = torch.Tensor([0.485, 0.456, 0.406]).cuda()\n",
    "std = torch.Tensor([0.229, 0.224, 0.225]).cuda()\n",
    "device = torch.device('cuda')\n",
    "\n",
    "def preprocess(image):\n",
    "    global device\n",
    "    device = torch.device('cuda')\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = PIL.Image.fromarray(image)\n",
    "    image = transforms.functional.to_tensor(image).to(device)\n",
    "    image.sub_(mean[:, None, None]).div_(std[:, None, None])\n",
    "    return image[None, ...]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "clf = make_pipeline(StandardScaler(), SVC(gamma='auto', kernel='rbf'))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "from preprocessdata import preprocessdata\n",
    "preprocessdata = preprocessdata(topology, num_parts)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "svm_train = False\n",
    "if svm_train:\n",
    "    clf, predicted = preprocessdata.trainsvm(clf, joints_train, joints_test, hand.labels_train, hand.labels_test)\n",
    "    filename = 'svmmodel.sav'\n",
    "    pickle.dump(clf, open(filename, 'wb'))\n",
    "else:\n",
    "    filename = 'svmmodel.sav'\n",
    "    clf = pickle.load(open(filename, 'rb'))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator StandardScaler from version 0.23.2 when using version 0.24.0. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator SVC from version 0.23.2 when using version 0.24.0. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator Pipeline from version 0.23.2 when using version 0.24.0. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "from jetcam.usb_camera import USBCamera\n",
    "from jetcam.csi_camera import CSICamera\n",
    "from jetcam.utils import bgr8_to_jpeg\n",
    "\n",
    "camera = USBCamera(width=WIDTH, height=HEIGHT, capture_fps=30, capture_device=0)\n",
    "#camera = CSICamera(width=WIDTH, height=HEIGHT, capture_fps=30)\n",
    "\n",
    "camera.running = True"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "def draw_joints(image, joints):\n",
    "    count = 0\n",
    "    for i in joints:\n",
    "        if i==[0,0]:\n",
    "            count+=1\n",
    "    if count>= 7:\n",
    "        return \n",
    "    for i in joints:\n",
    "        cv2.circle(image, (i[0],i[1]), 2, (0,0,255), 1)\n",
    "    cv2.circle(image, (joints[0][0],joints[0][1]), 2, (255,0,255), 1)\n",
    "    for i in hand_pose['skeleton']:\n",
    "        if joints[i[0]-1][0]==0 or joints[i[1]-1][0] == 0:\n",
    "            break\n",
    "        cv2.line(image, (joints[i[0]-1][0],joints[i[0]-1][1]), (joints[i[1]-1][0],joints[i[1]-1][1]), (0,255,0), 1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "with open('preprocess/gesture.json', 'r') as f:\n",
    "    gesture = json.load(f)\n",
    "gesture_type = gesture[\"paint\"]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "pen = []\n",
    "rectangle = []\n",
    "def draw(image, joints):\n",
    "    global pen\n",
    "    global rectangle\n",
    "    if preprocessdata.text==\"draw\":\n",
    "        pen.append((joints[6][0], joints[6][1]))\n",
    "    for i in range(len(pen)):\n",
    "        cv2.circle(image, pen[i], 1,(0,0,0), 2)\n",
    "    if preprocessdata.text==\"line\":\n",
    "        if joints[5]!=[0,0]:\n",
    "            rectangle.append((joints[6][0], joints[6][1]))\n",
    "    for i in range(len(rectangle)):\n",
    "        if i > 0:\n",
    "            if rectangle[i]!=[0,0]:\n",
    "                cv2.line(image,rectangle[i-1], rectangle[i], (0,0,0), 2)\n",
    "    \n",
    "    '''if preprocessdata.text==\"line\":\n",
    "        if joints[5]!=[0,0]:\n",
    "            rectangle.append((joints[6][0], joints[6][1]))\n",
    "    for i in range(len(rectangle)):\n",
    "        if i > 0:\n",
    "            if rectangle[i]!=[0,0]:\n",
    "                cv2.line(image,rectangle[i-1], rectangle[i], (255,255,255), 5)\n",
    "    '''\n",
    "    \n",
    "    if preprocessdata.text==\"erase\":\n",
    "        to_be_erased = []\n",
    "        for i in range(10):\n",
    "            for j in range(10):\n",
    "                \n",
    "                x = (joints[6][0]+i, joints[6][1]+j)\n",
    "                if x[0]>=0 or x[1]>=0:\n",
    "                    to_be_erased.append(x)\n",
    "        for i in to_be_erased:\n",
    "            if i in pen:\n",
    "                pen.remove(i)           \n",
    "            \n",
    "    if preprocessdata.text==\"clear\":\n",
    "        pen.clear()\n",
    "        rectangle.clear()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "rectangle_2 = []\n",
    "def draw_in_WB(image,joint):\n",
    "    if preprocessdata.text==\"line\":\n",
    "        if joints[5]!=[0,0]:\n",
    "            rectangle_2.append((joints[6][0], joints[6][1]))\n",
    "\n",
    "    if (len(rectangle)) > 0:\n",
    "        if rectangle[-1]!=[0,0]:\n",
    "            cv2.line(image,rectangle[-2], rectangle[-1], (0,0,0), 2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as anm\n",
    "\n",
    "img_white = np.full((224,224,3), 255, np.uint8)\n",
    "plt.imshow(img_white)\n",
    "plt.show()\n",
    "\n",
    "'''def update(i, img):\n",
    "    if i != 0:\n",
    "        plt.cla()\n",
    "    plt.imshow(img)\n",
    "    \n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "ani = anm.FuncAnimation(fig, update, fargs = ('Initial Animation! ', 2.0), \\\n",
    "    interval = 100, frames = 132)'''"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "image/png": "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",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ec48384e0>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     }
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\"def update(i, img):\\n    if i != 0:\\n        plt.cla()\\n    plt.imshow(img)\\n    \\nfig = plt.figure(figsize=(10, 10))\\nani = anm.FuncAnimation(fig, update, fargs = ('Initial Animation! ', 2.0),     interval = 100, frames = 132)\""
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "import ipywidgets\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "image_w = ipywidgets.Image(format='jpeg', width=224, height=224)\n",
    "display(image_w)"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d49e3920b4494ba2875c60e4a5463117",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image(value=b'', format='jpeg', height='224', width='224')"
      ]
     },
     "metadata": {}
    }
   ],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "def execute(change):\n",
    "    image = change['new']\n",
    "    data = preprocess(image)\n",
    "    cmap, paf = model_trt(data)\n",
    "    cmap, paf = cmap.detach().cpu(), paf.detach().cpu()\n",
    "    counts, objects, peaks = parse_objects(cmap, paf)\n",
    "    joints = preprocessdata.joints_inference(image, counts, objects, peaks)\n",
    "    #draw_objects(image, counts, objects, peaks )\n",
    "    draw_joints(image, joints)\n",
    "    dist_bn_joints = preprocessdata.find_distance(joints)\n",
    "    gesture = clf.predict([dist_bn_joints,[0]*num_parts*num_parts])\n",
    "    gesture_joints = gesture[0]\n",
    "    preprocessdata.prev_queue.append(gesture_joints)\n",
    "    preprocessdata.prev_queue.pop(0)\n",
    "    preprocessdata.print_label(image, preprocessdata.prev_queue, gesture_type)\n",
    "    draw(image, joints)\n",
    "    draw_in_WB(img_white, joints)\n",
    "    #image = image[:, ::-1, :]\n",
    "    image_w.value = bgr8_to_jpeg(image)\n",
    "    plt.imshow(img_white)\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "    \n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "execute({'new': camera.value})"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "image/png": "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",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ec48384a8>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ec2c7cb00>"
      ]
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "camera.observe(execute, names='value')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "camera.unobserve_all()"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ec03f8438>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Exception in thread Thread-4:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.6/threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/jetcam-0.0.0-py3.6.egg/jetcam/camera.py\", line 34, in _capture_frames\n",
      "    self.value = self._read()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/traitlets/traitlets.py\", line 585, in __set__\n",
      "    self.set(obj, value)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/traitlets/traitlets.py\", line 574, in set\n",
      "    obj._notify_trait(self.name, old_value, new_value)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/traitlets/traitlets.py\", line 1139, in _notify_trait\n",
      "    type='change',\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/traitlets/traitlets.py\", line 1176, in notify_change\n",
      "    c(change)\n",
      "  File \"<ipython-input-15-8ae0f618710f>\", line 20, in execute\n",
      "    plt.imshow(img_white)\n",
      "  File \"/usr/lib/python3/dist-packages/matplotlib/pyplot.py\", line 3104, in imshow\n",
      "    sci(ret)\n",
      "  File \"/usr/lib/python3/dist-packages/matplotlib/pyplot.py\", line 347, in sci\n",
      "    gca()._sci(im)\n",
      "  File \"/usr/lib/python3/dist-packages/matplotlib/axes/_base.py\", line 1688, in _sci\n",
      "    \"Argument must be an image, collection, or ContourSet in \"\n",
      "ValueError: Argument must be an image, collection, or ContourSet in this Axes\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "camera.running = False"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}